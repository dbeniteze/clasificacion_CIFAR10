{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aug_4cap_leaky_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/7cROnFW8gAQzUtmuyG+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbeniteze/clasificacion_CIFAR10/blob/master/aug_4cap_leaky_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTnLZ_oOqzGT",
        "colab_type": "code",
        "outputId": "725fa69d-871e-430a-d40d-332924893e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip3 install keras==2.3.1\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 8.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 12.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 12.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.17.5)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n",
            "TensorFlow 2.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-1eYc9I1U1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Larger CNN for the MNIST Dataset\n",
        "from keras import backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout, Reshape\n",
        "from keras.layers import Flatten, BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 32, 32, 3)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 32, 32, 3)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "#X_train = X_train / 255\n",
        "#X_test = X_test / 255\n",
        "\n",
        "X_train_mean = np.mean(X_train, axis=(0,1,2))\n",
        "X_train_std = np.std(X_train, axis=(0,1,2))\n",
        "X_train = (X_train - X_train_mean) / X_train_std\n",
        "X_test = (X_test - X_train_mean) / X_train_std\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "\n",
        "epoch = 150\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        "\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "LF=(5,5) # longitud del filtro\n",
        "model.add( Reshape((32,32,3), input_shape=(32,32,3)) )\n",
        "model.add( Conv2D(32, LF, strides=1, padding='same', activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.05))\n",
        "model.add( Conv2D(32, LF, strides=1, padding='same', activation='elu' ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D()) \n",
        "model.add(Dropout(0.05))\n",
        "model.add( Conv2D(64, LF, strides=1, padding='same', activation='elu' ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D()) \n",
        "model.add(Dropout(0.05))\n",
        "model.add( Conv2D(128, LF, strides=1, padding='same', activation='elu' ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())  \n",
        "model.add(Dropout(0.05))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add( Dense(128, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.05))\n",
        "model.add( Dense(64, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.05))\n",
        "model.add( Dense(32, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.05))\n",
        "model.add( Dense(10,activation='softmax') )\n",
        "  \n",
        "  \n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr_schedule(epoch)), metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4nJ7_jo2IbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Fit the model\n",
        "modelo = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch, batch_size=125)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "plt.plot(modelo.history['loss'])\n",
        "plt.plot(modelo.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yw-n0mUSoX8",
        "colab_type": "code",
        "outputId": "d55fb16e-a414-4561-b643-e9dedb2a4ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        #brillo\n",
        "        #brightness_range=[0.2,1.0],           #añadido\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=15,                          #modificado\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        #zoom_range=[0.5,1.0],                       #modificado\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "modelo = model.fit_generator(datagen.flow(X_train, y_train, batch_size=64),\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        epochs=epoch, verbose=1)\n",
        "    \n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "plt.plot(modelo.history['loss'])\n",
        "plt.plot(modelo.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "782/782 [==============================] - 44s 57ms/step - loss: 1.5348 - accuracy: 0.4561 - val_loss: 1.2111 - val_accuracy: 0.5702\n",
            "Epoch 2/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 1.1568 - accuracy: 0.5905 - val_loss: 0.9808 - val_accuracy: 0.6578\n",
            "Epoch 3/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 1.0051 - accuracy: 0.6485 - val_loss: 0.8965 - val_accuracy: 0.6950\n",
            "Epoch 4/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.9099 - accuracy: 0.6836 - val_loss: 0.8002 - val_accuracy: 0.7252\n",
            "Epoch 5/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.8450 - accuracy: 0.7061 - val_loss: 0.7871 - val_accuracy: 0.7314\n",
            "Epoch 6/150\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 0.7801 - accuracy: 0.7282 - val_loss: 0.7174 - val_accuracy: 0.7560\n",
            "Epoch 7/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.7479 - accuracy: 0.7405 - val_loss: 0.6850 - val_accuracy: 0.7702\n",
            "Epoch 8/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.7085 - accuracy: 0.7554 - val_loss: 0.7276 - val_accuracy: 0.7556\n",
            "Epoch 9/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.6780 - accuracy: 0.7674 - val_loss: 0.6583 - val_accuracy: 0.7780\n",
            "Epoch 10/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.6508 - accuracy: 0.7749 - val_loss: 0.6043 - val_accuracy: 0.7966\n",
            "Epoch 11/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.6333 - accuracy: 0.7812 - val_loss: 0.6251 - val_accuracy: 0.7896\n",
            "Epoch 12/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.6107 - accuracy: 0.7898 - val_loss: 0.5893 - val_accuracy: 0.8018\n",
            "Epoch 13/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.5912 - accuracy: 0.7984 - val_loss: 0.6247 - val_accuracy: 0.7901\n",
            "Epoch 14/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.5698 - accuracy: 0.8034 - val_loss: 0.5835 - val_accuracy: 0.8054\n",
            "Epoch 15/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.5618 - accuracy: 0.8082 - val_loss: 0.5685 - val_accuracy: 0.8058\n",
            "Epoch 16/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.5404 - accuracy: 0.8130 - val_loss: 0.5237 - val_accuracy: 0.8193\n",
            "Epoch 17/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.5345 - accuracy: 0.8156 - val_loss: 0.5304 - val_accuracy: 0.8192\n",
            "Epoch 18/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.5220 - accuracy: 0.8198 - val_loss: 0.5924 - val_accuracy: 0.8011\n",
            "Epoch 19/150\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 0.5141 - accuracy: 0.8238 - val_loss: 0.5288 - val_accuracy: 0.8227\n",
            "Epoch 20/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.4992 - accuracy: 0.8274 - val_loss: 0.5072 - val_accuracy: 0.8260\n",
            "Epoch 21/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.4930 - accuracy: 0.8295 - val_loss: 0.5120 - val_accuracy: 0.8233\n",
            "Epoch 22/150\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 0.4786 - accuracy: 0.8351 - val_loss: 0.5239 - val_accuracy: 0.8219\n",
            "Epoch 23/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.4716 - accuracy: 0.8370 - val_loss: 0.5071 - val_accuracy: 0.8273\n",
            "Epoch 24/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.4635 - accuracy: 0.8406 - val_loss: 0.5383 - val_accuracy: 0.8229\n",
            "Epoch 25/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.4540 - accuracy: 0.8418 - val_loss: 0.4891 - val_accuracy: 0.8367\n",
            "Epoch 26/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.4440 - accuracy: 0.8467 - val_loss: 0.5048 - val_accuracy: 0.8327\n",
            "Epoch 27/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4409 - accuracy: 0.8497 - val_loss: 0.4878 - val_accuracy: 0.8400\n",
            "Epoch 28/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4323 - accuracy: 0.8506 - val_loss: 0.4930 - val_accuracy: 0.8368\n",
            "Epoch 29/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4318 - accuracy: 0.8506 - val_loss: 0.5005 - val_accuracy: 0.8344\n",
            "Epoch 30/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4223 - accuracy: 0.8524 - val_loss: 0.4651 - val_accuracy: 0.8441\n",
            "Epoch 31/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4169 - accuracy: 0.8553 - val_loss: 0.4650 - val_accuracy: 0.8429\n",
            "Epoch 32/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.4096 - accuracy: 0.8576 - val_loss: 0.4528 - val_accuracy: 0.8477\n",
            "Epoch 33/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4047 - accuracy: 0.8598 - val_loss: 0.4302 - val_accuracy: 0.8561\n",
            "Epoch 34/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.4048 - accuracy: 0.8597 - val_loss: 0.4612 - val_accuracy: 0.8496\n",
            "Epoch 35/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3908 - accuracy: 0.8637 - val_loss: 0.4655 - val_accuracy: 0.8447\n",
            "Epoch 36/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.3872 - accuracy: 0.8663 - val_loss: 0.4731 - val_accuracy: 0.8399\n",
            "Epoch 37/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3850 - accuracy: 0.8668 - val_loss: 0.4821 - val_accuracy: 0.8414\n",
            "Epoch 38/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.3788 - accuracy: 0.8675 - val_loss: 0.4541 - val_accuracy: 0.8464\n",
            "Epoch 39/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.3797 - accuracy: 0.8673 - val_loss: 0.4408 - val_accuracy: 0.8511\n",
            "Epoch 40/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3748 - accuracy: 0.8705 - val_loss: 0.4548 - val_accuracy: 0.8479\n",
            "Epoch 41/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.3711 - accuracy: 0.8711 - val_loss: 0.4434 - val_accuracy: 0.8530\n",
            "Epoch 42/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3633 - accuracy: 0.8729 - val_loss: 0.4060 - val_accuracy: 0.8596\n",
            "Epoch 43/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3657 - accuracy: 0.8730 - val_loss: 0.4672 - val_accuracy: 0.8444\n",
            "Epoch 44/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3583 - accuracy: 0.8756 - val_loss: 0.4516 - val_accuracy: 0.8512\n",
            "Epoch 45/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3530 - accuracy: 0.8771 - val_loss: 0.4302 - val_accuracy: 0.8594\n",
            "Epoch 46/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3519 - accuracy: 0.8780 - val_loss: 0.4467 - val_accuracy: 0.8538\n",
            "Epoch 47/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3475 - accuracy: 0.8791 - val_loss: 0.4234 - val_accuracy: 0.8600\n",
            "Epoch 48/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3460 - accuracy: 0.8801 - val_loss: 0.4208 - val_accuracy: 0.8595\n",
            "Epoch 49/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.3415 - accuracy: 0.8825 - val_loss: 0.4397 - val_accuracy: 0.8533\n",
            "Epoch 50/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3458 - accuracy: 0.8818 - val_loss: 0.4254 - val_accuracy: 0.8589\n",
            "Epoch 51/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3361 - accuracy: 0.8821 - val_loss: 0.4320 - val_accuracy: 0.8580\n",
            "Epoch 52/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3320 - accuracy: 0.8838 - val_loss: 0.4322 - val_accuracy: 0.8585\n",
            "Epoch 53/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3323 - accuracy: 0.8840 - val_loss: 0.4486 - val_accuracy: 0.8542\n",
            "Epoch 54/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3223 - accuracy: 0.8895 - val_loss: 0.4239 - val_accuracy: 0.8626\n",
            "Epoch 55/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3251 - accuracy: 0.8875 - val_loss: 0.4509 - val_accuracy: 0.8572\n",
            "Epoch 56/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3236 - accuracy: 0.8869 - val_loss: 0.4322 - val_accuracy: 0.8612\n",
            "Epoch 57/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3191 - accuracy: 0.8897 - val_loss: 0.4809 - val_accuracy: 0.8452\n",
            "Epoch 58/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3132 - accuracy: 0.8905 - val_loss: 0.4253 - val_accuracy: 0.8606\n",
            "Epoch 59/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3177 - accuracy: 0.8901 - val_loss: 0.4124 - val_accuracy: 0.8634\n",
            "Epoch 60/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3126 - accuracy: 0.8906 - val_loss: 0.4319 - val_accuracy: 0.8625\n",
            "Epoch 61/150\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 0.3129 - accuracy: 0.8909 - val_loss: 0.4209 - val_accuracy: 0.8640\n",
            "Epoch 62/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3035 - accuracy: 0.8947 - val_loss: 0.4378 - val_accuracy: 0.8573\n",
            "Epoch 63/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3025 - accuracy: 0.8935 - val_loss: 0.4304 - val_accuracy: 0.8629\n",
            "Epoch 64/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.3095 - accuracy: 0.8935 - val_loss: 0.4556 - val_accuracy: 0.8532\n",
            "Epoch 65/150\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3026 - accuracy: 0.8957 - val_loss: 0.4223 - val_accuracy: 0.8642\n",
            "Epoch 66/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.2955 - accuracy: 0.8978 - val_loss: 0.4450 - val_accuracy: 0.8570\n",
            "Epoch 67/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.3026 - accuracy: 0.8943 - val_loss: 0.4193 - val_accuracy: 0.8644\n",
            "Epoch 68/150\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 0.2984 - accuracy: 0.8952 - val_loss: 0.4176 - val_accuracy: 0.8653\n",
            "Epoch 69/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.2920 - accuracy: 0.8969 - val_loss: 0.4379 - val_accuracy: 0.8594\n",
            "Epoch 70/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.2937 - accuracy: 0.8975 - val_loss: 0.4177 - val_accuracy: 0.8646\n",
            "Epoch 71/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.2886 - accuracy: 0.8998 - val_loss: 0.4393 - val_accuracy: 0.8615\n",
            "Epoch 72/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2815 - accuracy: 0.9010 - val_loss: 0.4523 - val_accuracy: 0.8583\n",
            "Epoch 73/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2868 - accuracy: 0.8995 - val_loss: 0.4102 - val_accuracy: 0.8718\n",
            "Epoch 74/150\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.2837 - accuracy: 0.9016 - val_loss: 0.4255 - val_accuracy: 0.8634\n",
            "Epoch 75/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2785 - accuracy: 0.9031 - val_loss: 0.4277 - val_accuracy: 0.8637\n",
            "Epoch 76/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2788 - accuracy: 0.9018 - val_loss: 0.4262 - val_accuracy: 0.8639\n",
            "Epoch 77/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2813 - accuracy: 0.9024 - val_loss: 0.4184 - val_accuracy: 0.8662\n",
            "Epoch 78/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2786 - accuracy: 0.9027 - val_loss: 0.4295 - val_accuracy: 0.8664\n",
            "Epoch 79/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2726 - accuracy: 0.9056 - val_loss: 0.4428 - val_accuracy: 0.8619\n",
            "Epoch 80/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2766 - accuracy: 0.9042 - val_loss: 0.4191 - val_accuracy: 0.8666\n",
            "Epoch 81/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2739 - accuracy: 0.9025 - val_loss: 0.4441 - val_accuracy: 0.8621\n",
            "Epoch 82/150\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.2700 - accuracy: 0.9053 - val_loss: 0.4183 - val_accuracy: 0.8686\n",
            "Epoch 83/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2673 - accuracy: 0.9064 - val_loss: 0.4218 - val_accuracy: 0.8665\n",
            "Epoch 84/150\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.2695 - accuracy: 0.9055 - val_loss: 0.4164 - val_accuracy: 0.8692\n",
            "Epoch 85/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2686 - accuracy: 0.9055 - val_loss: 0.4500 - val_accuracy: 0.8576\n",
            "Epoch 86/150\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.2683 - accuracy: 0.9066 - val_loss: 0.4079 - val_accuracy: 0.8708\n",
            "Epoch 87/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2655 - accuracy: 0.9068 - val_loss: 0.4346 - val_accuracy: 0.8627\n",
            "Epoch 88/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2601 - accuracy: 0.9084 - val_loss: 0.4596 - val_accuracy: 0.8577\n",
            "Epoch 89/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2599 - accuracy: 0.9088 - val_loss: 0.4230 - val_accuracy: 0.8688\n",
            "Epoch 90/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2530 - accuracy: 0.9119 - val_loss: 0.4138 - val_accuracy: 0.8718\n",
            "Epoch 91/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2615 - accuracy: 0.9087 - val_loss: 0.4270 - val_accuracy: 0.8664\n",
            "Epoch 92/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2552 - accuracy: 0.9100 - val_loss: 0.4231 - val_accuracy: 0.8707\n",
            "Epoch 93/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2527 - accuracy: 0.9115 - val_loss: 0.4456 - val_accuracy: 0.8633\n",
            "Epoch 94/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2532 - accuracy: 0.9119 - val_loss: 0.4203 - val_accuracy: 0.8712\n",
            "Epoch 95/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2535 - accuracy: 0.9113 - val_loss: 0.4262 - val_accuracy: 0.8685\n",
            "Epoch 96/150\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.2501 - accuracy: 0.9127 - val_loss: 0.4158 - val_accuracy: 0.8718\n",
            "Epoch 97/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2573 - accuracy: 0.9105 - val_loss: 0.4191 - val_accuracy: 0.8682\n",
            "Epoch 98/150\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.2459 - accuracy: 0.9133 - val_loss: 0.4323 - val_accuracy: 0.8662\n",
            "Epoch 99/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2469 - accuracy: 0.9151 - val_loss: 0.4668 - val_accuracy: 0.8568\n",
            "Epoch 100/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2486 - accuracy: 0.9131 - val_loss: 0.4296 - val_accuracy: 0.8682\n",
            "Epoch 101/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2497 - accuracy: 0.9125 - val_loss: 0.4308 - val_accuracy: 0.8674\n",
            "Epoch 102/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2491 - accuracy: 0.9135 - val_loss: 0.4171 - val_accuracy: 0.8703\n",
            "Epoch 103/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2459 - accuracy: 0.9141 - val_loss: 0.4291 - val_accuracy: 0.8682\n",
            "Epoch 104/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2430 - accuracy: 0.9154 - val_loss: 0.4340 - val_accuracy: 0.8659\n",
            "Epoch 105/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2426 - accuracy: 0.9163 - val_loss: 0.4247 - val_accuracy: 0.8685\n",
            "Epoch 106/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2435 - accuracy: 0.9159 - val_loss: 0.4345 - val_accuracy: 0.8683\n",
            "Epoch 107/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2394 - accuracy: 0.9164 - val_loss: 0.4171 - val_accuracy: 0.8713\n",
            "Epoch 108/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2407 - accuracy: 0.9162 - val_loss: 0.4371 - val_accuracy: 0.8650\n",
            "Epoch 109/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2369 - accuracy: 0.9162 - val_loss: 0.4436 - val_accuracy: 0.8653\n",
            "Epoch 110/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2366 - accuracy: 0.9174 - val_loss: 0.4278 - val_accuracy: 0.8663\n",
            "Epoch 111/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2356 - accuracy: 0.9170 - val_loss: 0.4167 - val_accuracy: 0.8732\n",
            "Epoch 112/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2328 - accuracy: 0.9187 - val_loss: 0.4317 - val_accuracy: 0.8692\n",
            "Epoch 113/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2361 - accuracy: 0.9179 - val_loss: 0.4408 - val_accuracy: 0.8672\n",
            "Epoch 114/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2339 - accuracy: 0.9177 - val_loss: 0.4200 - val_accuracy: 0.8701\n",
            "Epoch 115/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2281 - accuracy: 0.9196 - val_loss: 0.4093 - val_accuracy: 0.8728\n",
            "Epoch 116/150\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.2312 - accuracy: 0.9198 - val_loss: 0.4370 - val_accuracy: 0.8681\n",
            "Epoch 117/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2287 - accuracy: 0.9206 - val_loss: 0.4174 - val_accuracy: 0.8705\n",
            "Epoch 118/150\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.2285 - accuracy: 0.9198 - val_loss: 0.4246 - val_accuracy: 0.8704\n",
            "Epoch 119/150\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.2290 - accuracy: 0.9193 - val_loss: 0.4190 - val_accuracy: 0.8713\n",
            "Epoch 120/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.2247 - accuracy: 0.9225 - val_loss: 0.4265 - val_accuracy: 0.8699\n",
            "Epoch 121/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2278 - accuracy: 0.9201 - val_loss: 0.4171 - val_accuracy: 0.8749\n",
            "Epoch 122/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2269 - accuracy: 0.9216 - val_loss: 0.4202 - val_accuracy: 0.8743\n",
            "Epoch 123/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2282 - accuracy: 0.9184 - val_loss: 0.4423 - val_accuracy: 0.8661\n",
            "Epoch 124/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2232 - accuracy: 0.9212 - val_loss: 0.4161 - val_accuracy: 0.8705\n",
            "Epoch 125/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2195 - accuracy: 0.9218 - val_loss: 0.4215 - val_accuracy: 0.8697\n",
            "Epoch 126/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2245 - accuracy: 0.9213 - val_loss: 0.4324 - val_accuracy: 0.8672\n",
            "Epoch 127/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2241 - accuracy: 0.9220 - val_loss: 0.4262 - val_accuracy: 0.8691\n",
            "Epoch 128/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2214 - accuracy: 0.9224 - val_loss: 0.4210 - val_accuracy: 0.8725\n",
            "Epoch 129/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2205 - accuracy: 0.9225 - val_loss: 0.4307 - val_accuracy: 0.8712\n",
            "Epoch 130/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2220 - accuracy: 0.9224 - val_loss: 0.4095 - val_accuracy: 0.8733\n",
            "Epoch 131/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2183 - accuracy: 0.9236 - val_loss: 0.4484 - val_accuracy: 0.8646\n",
            "Epoch 132/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2169 - accuracy: 0.9245 - val_loss: 0.4405 - val_accuracy: 0.8646\n",
            "Epoch 133/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2180 - accuracy: 0.9230 - val_loss: 0.4383 - val_accuracy: 0.8655\n",
            "Epoch 134/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2156 - accuracy: 0.9251 - val_loss: 0.4411 - val_accuracy: 0.8678\n",
            "Epoch 135/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2216 - accuracy: 0.9226 - val_loss: 0.4163 - val_accuracy: 0.8731\n",
            "Epoch 136/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2160 - accuracy: 0.9232 - val_loss: 0.4264 - val_accuracy: 0.8726\n",
            "Epoch 137/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2122 - accuracy: 0.9256 - val_loss: 0.4181 - val_accuracy: 0.8712\n",
            "Epoch 138/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2112 - accuracy: 0.9249 - val_loss: 0.4291 - val_accuracy: 0.8688\n",
            "Epoch 139/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2129 - accuracy: 0.9249 - val_loss: 0.4200 - val_accuracy: 0.8720\n",
            "Epoch 140/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2172 - accuracy: 0.9239 - val_loss: 0.4336 - val_accuracy: 0.8687\n",
            "Epoch 141/150\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.2104 - accuracy: 0.9252 - val_loss: 0.4325 - val_accuracy: 0.8698\n",
            "Epoch 142/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2111 - accuracy: 0.9258 - val_loss: 0.4341 - val_accuracy: 0.8701\n",
            "Epoch 143/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2122 - accuracy: 0.9248 - val_loss: 0.4381 - val_accuracy: 0.8680\n",
            "Epoch 144/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2129 - accuracy: 0.9262 - val_loss: 0.4119 - val_accuracy: 0.8749\n",
            "Epoch 145/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2080 - accuracy: 0.9271 - val_loss: 0.4223 - val_accuracy: 0.8713\n",
            "Epoch 146/150\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.2078 - accuracy: 0.9268 - val_loss: 0.4422 - val_accuracy: 0.8684\n",
            "Epoch 147/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2080 - accuracy: 0.9268 - val_loss: 0.3931 - val_accuracy: 0.8801\n",
            "Epoch 148/150\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.2111 - accuracy: 0.9259 - val_loss: 0.4210 - val_accuracy: 0.8744\n",
            "Epoch 149/150\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.2077 - accuracy: 0.9272 - val_loss: 0.4322 - val_accuracy: 0.8730\n",
            "Epoch 150/150\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.2084 - accuracy: 0.9275 - val_loss: 0.4255 - val_accuracy: 0.8738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Error: 12.62%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}